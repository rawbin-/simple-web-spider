# simple-web-spider
微信公众号看到了一些书的连载，章节太多，不适合手动拷贝，于是准备搞个简单工具来弄一下
网上找了些工具，要么就是不（太）会（垃）用（垃），要么就是要收费或者限制数量

# 基本需求
- 把`https://mp.weixin.qq.com/s/1kt9PVoonTsS2vmY9rWLxQ` 这里面的这些连载电子书下载下来
- 同时支持单本下载，多本下载等
- 同时支持下载离线html，生成pdf，docx等


# 实现过程
## 选型
- 前端Node平台下，也没什么好选的，`puppeteer`，首选，也算是对我早年的`Web自动化之Headless Chrome`系列的一个补充，那会`puppeteer`基本还不算可用的状态

## 开搞v0.1
- 先撸一版能用的，直接是多本下载的模式
- 思路比较简单
    - 打开总索引页面
    - 选择每个书索引及链接
    - 分别打开每个书索引链接，如法炮制，打开每个章节的链接，并获取页面内容，存入html文件
    
## 检查比较
### 查看产物文件目录
- 发现有本书章节内容没有在目录中
- 发现有些章节名是空
- 发现有些书下载下来只有一个章节
- 有些文章有图，有音频，没抓下来（还是在线访问的状态）

### 原因分析
- 发现有本书章节内容没有在目录中，原因是有链接却没有链接对应的文本，这个要过滤掉
- 发现有些章节名是空，原因是有链接却没有链接对应的文本，这个要过滤掉
- 发现有些书下载下来只有一个章节，是因为书目索引下，还是一个书目索引，索引下跳到了话题目录，这个要新增解析规则
- 有些文章有图，有音频，没抓下来（还是在线访问的状态），这个需要单独抽取资源


## 迭代更新
### v0.3
- 增加对空链的过滤，直接打日志或者检查目录可校验

